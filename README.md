# NEAGCL

Graph Neural Networks (GNNs) are capable of modeling graph data using various types of nodes and edges, and thus can be widely used in the fields of recommender systems, bioinformatics. However, most existing graph neural network models do not alleviate the problem of incomplete raw graph data and the need for extensive labels.
This leads to increased labor costs and produces sub-optimal or even incorrect results. In this paper, we propose an end-to-end method for fusing Learnable Node-level and Edge-level Augmentation in Automatic Graph-level Contrastive Learning (NEAGCL), consisting of a graph preprocessing module and tailored graph-level contrastive learning. 
In the graph preprocessing module, we propose a parallel view generator method, which solves the problem of data incompleteness by simultaneously performing adaptive structure enhancement and learnable node information optimization. In tailored graph-level contrastive learning, we employ contrast learning in self-supervised learning and classification loss to jointly train graph classifiers, thus solving the problem of relying on the original labels.
Besides, our model employs differentiable contrastive learning and is an end-to-end self-supervised model. Comparative tests of semi-supervised learning and unsupervised learning on seven benchmark datasets for graph-level tasks demonstrate the superiority of our model. 
We demonstrate that effective graph comparison learning requires learning better joint representations of graph structures and node attributes to improve the performance of graph-level tasks.
